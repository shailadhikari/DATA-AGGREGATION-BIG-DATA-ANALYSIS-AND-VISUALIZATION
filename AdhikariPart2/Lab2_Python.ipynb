{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------------------------Lab 2 Python-------------------------------------------------\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from urllib.request import urlopen\n",
    "\n",
    "#api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=Trump&begin_date=20180402&end_date=20180403\"\n",
    "#data = urlopen(api_url)\n",
    "#response = data.read()\n",
    "#data_json = json.loads(response)\n",
    "#print(data_json)\n",
    "#print(json['status'])\n",
    "#body = response[559:1000]\n",
    "\n",
    "# Add your 'print' statement here!\n",
    "#print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'Trump'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=Trump&begin_date=20180402&end_date=20180403\"\n",
    "#data = urlopen(api_url)\n",
    "#response = data.read()\n",
    "#Total pages available in the NY Times for mentioned parameters\n",
    "page = 12\n",
    "count = 0\n",
    "writer = csv.writer(open('NYTrump20180402_20180403.csv', 'w'))\n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   time.sleep(5)\n",
    "   load_data = urlopen(api_url)\n",
    "   response = load_data.read()\n",
    "   json_data = json.loads(response)\n",
    "   #print(j_data1)\n",
    "   for i in json_data['response']['docs']:\n",
    "        #np.savetxt('data.csv', (i['snippet'], i['web_url'], i['headline']['main']), delimiter=',')\n",
    "        snippet = i['snippet'].encode('utf-8')\n",
    "        weburl = i['web_url'].encode('utf-8')\n",
    "        headline = i['headline']['main'].encode('utf-8')\n",
    "        writer.writerow([snippet, weburl, headline])\n",
    "        #print('#####')\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "\n",
    "#data_json = json.loads(response)\n",
    "#print(data_json)\n",
    "#print(json['status'])\n",
    "#body = response[559:1000]\n",
    "\n",
    "# Add your 'print' statement here!\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'Trump'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    " \n",
    "\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=Trump&begin_date=20180402&end_date=20180403\"\n",
    "#data = urlopen(api_url)\n",
    "#response = data.read()\n",
    "page = 12\n",
    "count = 0\n",
    "#writer = csv.writer(open('data.csv', 'w'))\n",
    "file = open('FileNYTrump_Raw.txt','wb') \n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   #print (api_url)\n",
    "   time.sleep(5)\n",
    "   data_url = urlopen(api_url)\n",
    "   response_url = data_url.read()\n",
    "   json_url_data = json.loads(response_url)\n",
    "   #print(j_data1)\n",
    "   #with open(\"file.txt\", \"w\") as output:\n",
    "   for i in json_url_data['response']['docs']:\n",
    "        #np.savetxt('data.csv', (i['snippet'], i['web_url'], i['headline']['main']), delimiter=',')\n",
    "        #print(i['snippet'])\n",
    "        #print(i['web_url'])\n",
    "        #print(i['headline']['main'])\n",
    "        weburl = i['web_url']\n",
    "        html = urllib.request.urlopen(weburl)\n",
    "        #soup = BeautifulSoup(html, \"lxml\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        #data2 = soup.findAll(text=True)\n",
    "        ptags = soup.find_all(\"p\",text=True)   \n",
    "        for par in ptags:\n",
    "            file.write((par.get_text(strip=True)).encode('utf8'))\n",
    "        #result2 = filter(visible, data2)\n",
    "        #output.write(str(list(result2)))\n",
    "        #newdata = re.sub('[^a-zA-Z ]','',str(data2)).lower()\n",
    "        #print(list(result2))\n",
    "        #print('#####')\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "file.close()\n",
    "\n",
    "#data_json = json.loads(response)\n",
    "#print(data_json)\n",
    "#print(json['status'])\n",
    "#body = response[559:1000]\n",
    "\n",
    "# Add your 'print' statement here!\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'H1B'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=H1B\"\n",
    "#data = urlopen(api_url)\n",
    "#response = data.read()\n",
    "#Total pages available in the NY Times for mentioned parameters\n",
    "page = 12\n",
    "count = 0\n",
    "writer = csv.writer(open('NYH1B_Page_11.csv', 'w'))\n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   time.sleep(5)\n",
    "   load_data = urlopen(api_url)\n",
    "   response = load_data.read()\n",
    "   json_data = json.loads(response)\n",
    "   #print(j_data1)\n",
    "   for i in json_data['response']['docs']:\n",
    "        #np.savetxt('data.csv', (i['snippet'], i['web_url'], i['headline']['main']), delimiter=',')\n",
    "        snippet = i['snippet'].encode('utf-8')\n",
    "        weburl = i['web_url'].encode('utf-8')\n",
    "        headline = i['headline']['main'].encode('utf-8')\n",
    "        writer.writerow([snippet, weburl, headline])\n",
    "        #print('#####')\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "\n",
    "#data_json = json.loads(response)\n",
    "#print(data_json)\n",
    "#print(json['status'])\n",
    "#body = response[559:1000]\n",
    "\n",
    "# Add your 'print' statement here!\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'H1B'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    " \n",
    "\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=H1B\"\n",
    "page = 12\n",
    "count = 0\n",
    "file = open('FileNYH1B_Raw.txt','wb') \n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   #print(api_url)\n",
    "   #print('Initial')\n",
    "   time.sleep(5)\n",
    "   try:\n",
    "       #print(api_url)\n",
    "       #print('Middle')\n",
    "       data_url = urlopen(api_url)\n",
    "       response_url = data_url.read()\n",
    "       json_url_data = json.loads(response_url)\n",
    "       #print(api_url)\n",
    "       #print('\\n')\n",
    "   except:\n",
    "       pass\n",
    "   for i in json_url_data['response']['docs']:\n",
    "        weburl = i['web_url']\n",
    "        try:\n",
    "            html = urllib.request.urlopen(weburl)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            ptags = soup.find_all(\"p\",text=True)   \n",
    "            for par in ptags:\n",
    "                file.write((par.get_text(strip=True)).encode('utf8'))\n",
    "        except:\n",
    "            pass\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'Immigration'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=Immigration&begin_date=20180328&end_date=20180403\"\n",
    "#data = urlopen(api_url)\n",
    "#response = data.read()\n",
    "#Total pages available in the NY Times for mentioned parameters\n",
    "page = 13\n",
    "count = 0\n",
    "writer = csv.writer(open('NYImmigration20180402_20180403_Page12.csv', 'w'))\n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   time.sleep(5)\n",
    "   load_data = urlopen(api_url)\n",
    "   response = load_data.read()\n",
    "   json_data = json.loads(response)\n",
    "   #print(j_data1)\n",
    "   for i in json_data['response']['docs']:\n",
    "        #np.savetxt('data.csv', (i['snippet'], i['web_url'], i['headline']['main']), delimiter=',')\n",
    "        snippet = i['snippet'].encode('utf-8')\n",
    "        weburl = i['web_url'].encode('utf-8')\n",
    "        headline = i['headline']['main'].encode('utf-8')\n",
    "        writer.writerow([snippet, weburl, headline])\n",
    "        #print('#####')\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "\n",
    "#data_json = json.loads(response)\n",
    "#print(data_json)\n",
    "#print(json['status'])\n",
    "#body = response[559:1000]\n",
    "\n",
    "# Add your 'print' statement here!\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using NYTimes API to fetch the results for tag 'Immigration'\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    " \n",
    "\n",
    "api_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=e01288d6543146f594717ec949cdb61b&q=Immigration&begin_date=20180328&end_date=20180403\"\n",
    "page = 13\n",
    "count = 0\n",
    "file = open('FileNYImmigration_Raw.txt','wb') \n",
    "while (count < page):\n",
    "   api_url = api_url + \"&page=\" + str(count)\n",
    "   #print(api_url)\n",
    "   #print('Initial')\n",
    "   time.sleep(5)\n",
    "   try:\n",
    "       #print(api_url)\n",
    "       #print('Middle')\n",
    "       data_url = urlopen(api_url)\n",
    "       response_url = data_url.read()\n",
    "       json_url_data = json.loads(response_url)\n",
    "       #print(api_url)\n",
    "       #print('\\n')\n",
    "   except:\n",
    "       pass\n",
    "   for i in json_url_data['response']['docs']:\n",
    "        weburl = i['web_url']\n",
    "        try:\n",
    "            html = urllib.request.urlopen(weburl)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            ptags = soup.find_all(\"p\",text=True)   \n",
    "            for par in ptags:\n",
    "                file.write((par.get_text(strip=True)).encode('utf8'))\n",
    "        except:\n",
    "            pass\n",
    "   api_url = api_url.partition(\"&page=\")[0]\n",
    "   count = count + 1\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
